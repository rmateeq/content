{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro to Data Science (Fall2023)\n",
    "#### Week10 (18/18/19-Dec-2023)\n",
    "#### Week11 (~2/4-Jan-2024)\n",
    "\n",
    "**M Ateeq**,<br>\n",
    "*Department of Data Science, The Islamia University of Bahawalpur.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Life Cycle of a Data-Driven Task/Project**\n",
    "\n",
    "1. ~~Define the Problem~~\n",
    "2. ~~Collect Data~~\n",
    "3. **Explore and Understand Data**\n",
    "4. **Preprocess Data**\n",
    "5. Perform Data Analysis\n",
    "6. Feature Engineering\n",
    "7. Select a Model\n",
    "8. Train the Model\n",
    "9. Evaluate Model Performance\n",
    "10. Tune Hyperparameters\n",
    "11. Deploy the Model\n",
    "12. Monitor and Maintain the Model\n",
    "13. Iterate and Improve\n",
    "\n",
    "Keep in mind that these steps are often iterative, and the process may loop back to earlier steps as needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:3px;border-width:0;color:blue;background-color:blue\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Some Important Libraries**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1. Numpy**\n",
    "\n",
    "**A Beginner's Guide to Numbers on Steroids**\n",
    "\n",
    "NumPy, short for **Numerical Python**, is a powerful Python library that takes data manipulation to the next level. Think of it as a supercharged toolbox for dealing with numbers in Python. Here's a quick overview for beginners:\n",
    "\n",
    "**What does it do?**\n",
    "\n",
    "* **Stores and manipulates multi-dimensional arrays:** NumPy provides efficient ways to work with data organized in grids or tables, called arrays. These arrays can have multiple dimensions, like rows and columns, making them ideal for scientific data, images, and other complex datasets.\n",
    "* **Performs fast calculations:** NumPy uses optimized code written in C, making it significantly faster than standard Python loops for common mathematical operations like sums, averages, and matrix manipulations.\n",
    "* **Offers built-in functions:** NumPy comes loaded with a vast library of functions for tasks like sorting, searching, and statistical analysis, saving you the hassle of writing repetitive code.\n",
    "* **Enables interoperability:** NumPy seamlessly integrates with other scientific and data analysis libraries like SciPy and pandas, making it a cornerstone of the Python data science ecosystem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why use NumPy?**\n",
    "\n",
    "* **Speed & Efficiency:** NumPy significantly outperforms standard Python loops for mathematical operations on large datasets, saving you time and resources.\n",
    "* **Conciseness & Readability:** NumPy functions and methods offer a concise and elegant way to express complex mathematical operations, making your code easier to read and maintain.\n",
    "* **Scalability & Power:** NumPy's multi-dimensional arrays and efficient codebase make it ideal for working with large and complex datasets, preparing you for advanced data analysis tasks.\n",
    "\n",
    "**How to get started:**\n",
    "\n",
    "* **Import the library:** Simply add `import numpy as np` at the beginning of your Python code.\n",
    "* **Create arrays:** Use `np.array()` to create arrays from lists, other arrays, or even functions.\n",
    "* **Explore available functions:** The NumPy documentation provides a comprehensive list of functions for various tasks. \n",
    "* **Practice & Experiment:** Start with simple operations and gradually explore more advanced features as you gain confidence.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remember:**\n",
    "\n",
    "* NumPy is not a replacement for basic Python loops and operations. Use it when performance and conciseness matter for calculations with multi-dimensional data.\n",
    "* The learning curve for NumPy can be steep, but the rewards are significant for efficient data analysis and scientific computing.\n",
    "\n",
    "With a little effort, NumPy can become your go-to tool for handling numbers in Python, opening doors to exciting avenues in data science and beyond!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2. Pandas**\n",
    "\n",
    "**Wrangling Data with Power and Panache**\n",
    "\n",
    "If NumPy is the muscle behind data manipulation in Python, then **pandas** is the brain, bringing structure and meaning to your numbers. Think of it as a spreadsheet on steroids, packed with tools to organize, analyze, and explore your data with ease.\n",
    "\n",
    "**What does it do?**\n",
    "\n",
    "* **Reads and writes various data formats:** Pandas imports data from popular sources like CSV, Excel, and SQL databases, making it a versatile data intake point.\n",
    "* **Structures data in DataFrames:** Data is organized into \"DataFrames\", which are essentially tables with rows, columns, and labeled headers. This provides a clear and easily manageable structure for analysis.\n",
    "* **Offers powerful data manipulation tools:** Pandas provides a vast array of functions and methods for filtering, sorting, grouping, aggregating, and merging data, allowing you to extract insights and answer questions about your data.\n",
    "* **Visualizes data with ease:** Pandas comes bundled with data visualization capabilities, allowing you to create charts, graphs, and plots directly from your DataFrame, transforming numbers into compelling visual stories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why use pandas?**\n",
    "\n",
    "* **Organization & Clarity:** Pandas brings order to messy data, making it easier to understand, analyze, and share with others.\n",
    "* **Efficiency & Power:** Common data manipulation tasks become effortless with pandas' built-in functions, saving you time and code compared to manual loops.\n",
    "* **Flexibility & Versatility:** Pandas works with various data types and structures, from numerical data to text and dates, making it a one-stop shop for most data analysis needs.\n",
    "* **Data Visualization for Everyone:** No need for separate visualization libraries! Pandas lets you create insightful visuals directly from your data, perfect for presentations and reports."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How to get started:**\n",
    "\n",
    "* **Import the library:** Simply add `import pandas as pd` at the beginning of your Python code.\n",
    "* **Read data:** Use `pd.read_csv()`, `pd.read_excel()`, or similar functions to import data from various sources.\n",
    "* **Explore your DataFrame:** Use methods like `head()`, `tail()`, and `info()` to get a snapshot of your data's structure and contents.\n",
    "* **Learn common operations:** Practice filtering, sorting, grouping, and other data manipulation tasks using pandas' extensive library of functions.\n",
    "* **Dive into visualization:** Use plotting methods like `plot()`, `bar()`, and `scatter()` to create different types of charts and graphs directly from your DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remember:**\n",
    "\n",
    "* Pandas is not just about tables. It's a powerful tool for exploring, cleaning, and analyzing data, opening doors to advanced data science techniques.\n",
    "* The learning curve for pandas can be gradual. Start with basic operations and gradually build your skills with practice and exploration.\n",
    "\n",
    "With a bit of effort, pandas can become your best friend for data wrangling in Python, empowering you to unlock the secrets hidden within your data and tell its story with clarity and insight. So, dive in, explore, and discover the power of pandas!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3. Scikit-Learn**\n",
    "\n",
    "**Your Superpower for Machine Learning in Python**\n",
    "\n",
    "Imagine a world where you can train computers to learn from data and make predictions – that's the magic of **scikit-learn**, a Python library brimming with tools to turn your data into intelligent models. Think of it as a laboratory filled with powerful algorithms and ready-to-use tools for exploring the fascinating realm of machine learning.\n",
    "\n",
    "**What does it do?**\n",
    "\n",
    "* **Offers a plethora of algorithms:** Scikit-learn boasts a vast collection of machine learning algorithms like regression, classification, clustering, and dimensionality reduction. You can choose the right tool for the task at hand, from predicting house prices to classifying images or identifying customer segments.\n",
    "* **Simplifies model training and evaluation:** With scikit-learn, training and testing your models is just a few lines of code away. It handles data pre-processing, model training, optimization, and evaluation metrics, letting you focus on the bigger picture.\n",
    "* **Provides visual analysis tools:** Scikit-learn offers handy visualization tools to explore your data, understand model behavior, and diagnose potential issues. This visual feedback helps you refine your models and gain deeper insights.\n",
    "* **Enables interoperability:** Scikit-learn seamlessly integrates with other popular Python libraries like NumPy and pandas, making it a cornerstone of the data science ecosystem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why use scikit-learn?**\n",
    "\n",
    "* **Accessibility & Ease of Use:** Scikit-learn's well-documented interface and clear syntax make it approachable even for beginners, allowing you to jump into machine learning without getting bogged down in complex coding.\n",
    "* **Efficiency & Time-Saving:** Its pre-built algorithms and tools save you from writing complex code from scratch, allowing you to focus on model selection, optimization, and analysis.\n",
    "* **Flexibility & Scalability:** With a wide range of algorithms and options for customization, scikit-learn can handle diverse data and learning tasks, growing with your data science skills.\n",
    "* **Community & Support:** A vibrant community of data scientists and developers around scikit-learn provide extensive documentation, tutorials, and support, ensuring you're never lost in the machine learning maze."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How to get started:**\n",
    "\n",
    "* **Import the library:** Simply add `import sklearn` at the beginning of your Python code.\n",
    "* **Choose your algorithm:** Explore the scikit-learn documentation to find the right algorithm for your problem.\n",
    "* **Load and prepare your data:** Use pandas and NumPy to import and pre-process your data for your chosen algorithm.\n",
    "* **Train and evaluate your model:** With a few lines of code, train your model, evaluate its performance, and fine-tune it for better results.\n",
    "* **Visualize and interpret:** Use scikit-learn's visualization tools to gain insights into your model's behavior and understand its predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remember:**\n",
    "\n",
    "* Scikit-learn is a powerful tool, but it takes practice and understanding to leverage its full potential. Start with simple applications and gradually build your skills with each project.\n",
    "* Don't just throw data at algorithms – understanding the problem you're trying to solve and selecting the right approach is crucial for success.\n",
    "* Resources are abundant! Utilize the extensive documentation, tutorials, and online communities to learn, experiment, and grow your machine learning expertise.\n",
    "\n",
    "With scikit-learn as your guide, you can delve into the exciting world of machine learning, empower your data with predictive power, and unlock the potential for intelligent solutions to real-world problems. So, take the first step, explore, and unleash the data scientist within!\n",
    "\n",
    "I hope these brief overviews of NumPy, pandas, and scikit-learn provide a good starting point for your exploration of these foundational libraries. Feel free to ask any further questions you might have!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4. Matplotlib**\n",
    "\n",
    "**Painting Pictures with Your Data in Python**\n",
    "\n",
    "Imagine turning your numbers into mesmerizing visualizations, telling stories with vibrant colors and captivating shapes – that's the magic of **matplotlib**, a Python library that transforms your data into stunning visual masterpieces. Think of it as your personal art studio, filled with brushes, paints, and canvases, ready to bring your data to life through the power of visualization.\n",
    "\n",
    "**What does it do?**\n",
    "\n",
    "* **Creates diverse plots and charts:** Matplotlib boasts a vast collection of plot types, from classic line graphs and bar charts to intricate heatmaps and boxplots. You can choose the perfect visual expression for any type of data and analysis.\n",
    "* **Offers customizability and control:** Matplotlib gives you granular control over every aspect of your visualization, from colors and axes to labels and legends. You can tailor the visual story to your specific needs and audience.\n",
    "* **Enables interactive plots:** Breathe life into your visualizations with interactive features like zooming, panning, and tooltips. This allows viewers to explore the data dynamically and gain deeper insights.\n",
    "* **Integrates with other libraries:** Matplotlib seamlessly works with popular Python libraries like NumPy and pandas, making it easy to plot data directly from your data structures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why use matplotlib?**\n",
    "\n",
    "* **Communication & Storytelling:** Matplotlib helps you communicate complex data insights in a clear and compelling way, making your data accessible and engaging for diverse audiences.\n",
    "* **Exploration & Analysis:** Visualizing your data reveals patterns and trends that might be hidden within numbers, guiding your analysis and leading to new discoveries.\n",
    "* **Presentation & Impact:** Stunning visualizations can elevate your presentations and reports, leaving a lasting impression on your audience and increasing the impact of your findings.\n",
    "* **Customization & Flexibility:** Matplotlib offers endless possibilities for customization, allowing you to create unique and expressive visuals that perfectly capture your data's essence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How to get started:**\n",
    "\n",
    "* **Import the library:** Simply add `import matplotlib.pyplot as plt` at the beginning of your Python code.\n",
    "* **Choose your plot type:** Explore the matplotlib documentation to find the right plot for your data and analysis.\n",
    "* **Prepare your data:** Use NumPy or pandas to format your data in a way that's compatible with your chosen plot type.\n",
    "* **Create your visualization:** With a few lines of code, you can generate your plot, customize its elements, and add labels and titles.\n",
    "* **Save or display your masterpiece:** Export your visualization as an image or show it directly in your code for interactive exploration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remember:**\n",
    "\n",
    "* Matplotlib is a powerful tool, but it takes practice to unleash its full potential. Start with simple plots and gradually build your skills with each visualization.\n",
    "* Choose the right plot type for your data and audience. A cluttered visualization can obscure insights rather than reveal them.\n",
    "* Don't be afraid to experiment! Matplotlib offers endless possibilities for creative expression, so let your data guide your artistic vision.\n",
    "\n",
    "With matplotlib as your paintbrush, you can transform your data into captivating stories, unlock deeper understanding, and inspire others with the beauty hidden within your numbers. So, dive in, explore, and unleash your inner data artist!\n",
    "\n",
    "I hope these overviews of these important Python libraries have been helpful! Feel free to ask any specific questions you have about them or any other data science topics. I'm always happy to assist!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:3px;border-width:0;color:blue;background-color:blue\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore and Understand the California Housing Data\n",
    "\n",
    "**Introduction:**\n",
    "\n",
    "This session will delve into the California Housing dataset, a popular example in scikit-learn for regression tasks. We'll embark on a journey to uncover its secrets, understand its features, and prepare it for effective machine learning.\n",
    "\n",
    "**Learning Objectives:**\n",
    "\n",
    "* Load and access the California Housing data using scikit-learn.\n",
    "* Explore the data's basic characteristics: size, shape, data types.\n",
    "* Analyze descriptive statistics: measures of central tendency, dispersion, and correlation.\n",
    "* Visualize the data distribution using histograms, boxplots, and scatter plots.\n",
    "* Identify and handle potential data quality issues like missing values or outliers.\n",
    "* Gain insights into the relationships between features and the target variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **California Housing Price Dataset: A Quick Dive**\n",
    "\n",
    "**Origin:**\n",
    "\n",
    "- This dataset originates from a 1990 California State Department of Finance study examining median housing prices in California census tracts.\n",
    "- It comprises data collected from around 20,000 houses in various regions throughout the state.\n",
    "\n",
    "**Purpose:**\n",
    "\n",
    "- Initially used for analyzing factors influencing housing prices across California.\n",
    "- Widely adopted in the machine learning community as a benchmark dataset for regression tasks.\n",
    "- Useful for practicing and testing various supervised learning algorithms and techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Features:**\n",
    "\n",
    "- The dataset contains 8 numerical features describing attributes of each census tract:\n",
    "    - Median income in block group\n",
    "    - Housing density\n",
    "    - Percentage of non-retail land\n",
    "    - Median age of owner-occupied units\n",
    "    - Distance to nearest major roads\n",
    "    - Index of accessibility to radial highways\n",
    "    - Full tax value per $10,000\n",
    "    - Average number of rooms per dwelling\n",
    "    \n",
    "**Target Variable:**\n",
    "\n",
    "- The target variable is the median house price in each census tract, measured in thousands of dollars."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. What organization originally collected the data in the California Housing Price dataset?**\n",
    "    - a) U.S. Census Bureau\n",
    "    - b) California State Department of Finance\n",
    "    - c) National Bureau of Economic Research\n",
    "    \n",
    "<details>\n",
    "<summary>Click to reveal the answer:</summary>\n",
    "    b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. What type of machine learning task is primarily suited for this dataset?**\n",
    "    - a) Classification\n",
    "    - b) Regression\n",
    "    - c) Clustering\n",
    "    \n",
    "<details>\n",
    "<summary>Click to reveal the answer:</summary>\n",
    "    b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. How many features are included in the dataset?**\n",
    "    - a) 5\n",
    "    - b) 8\n",
    "    - c) 12\n",
    "\n",
    "<details>\n",
    "<summary>Click to reveal the answer:</summary>\n",
    "    b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Use `sklearn.datasets.fetch_california_housing()` to Load the Dataset**\n",
    "\n",
    "**1. Importing the libraries:**\n",
    "\n",
    "Start by importing the necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_california_housing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Loading the California Housing data:**\n",
    "\n",
    "Use the `fetch_california_housing()` function to load the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "california_housing = fetch_california_housing()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Exploring the returned object:**\n",
    "\n",
    "The `fetch_california_housing()` function returns a dictionary-like object containing various attributes:\n",
    "\n",
    "* `data`: A 2D array containing the feature values for each data point.\n",
    "* `target`: A 1D array containing the target variable values (median house price).\n",
    "* `feature_names`: A list containing the names of each feature.\n",
    "* `DESCR`: A string containing a description of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Understanding the data properties:**\n",
    "\n",
    "Use the following commands to examine the data:\n",
    "\n",
    "* Print the data shape: `print(california_housing.data.shape)` (This will tell you the number of data points and features).\n",
    "* Print the feature names: `print(california_housing.feature_names)` (This will show you the names of each feature).\n",
    "* Get a preview of the data: `print(pd.DataFrame(california_housing.data)[:5])` (This will display the first five rows of the data table)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. What function in `sklearn` is used to load the California Housing dataset?**\n",
    "    - a) `load_california_housing()`\n",
    "    - b) `fetch_california_housing()`\n",
    "    - c) `get_california_housing()`\n",
    "\n",
    "<details>\n",
    "<summary>Click to reveal the answer:</summary>\n",
    "    b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. What information does the `DESCR` attribute of the returned object contain?**\n",
    "    - a) Data shape and size\n",
    "    - b) Feature names\n",
    "    - c) Description of the dataset\n",
    "\n",
    "<details>\n",
    "<summary>Click to reveal the answer:</summary>\n",
    "    c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. How do you access the feature names of the California Housing data?**\n",
    "    - a) `california_housing.features`\n",
    "    - b) `california_housing.feature_names`\n",
    "    - c) `california_housing.data.names`\n",
    "\n",
    "<details>\n",
    "<summary>Click to reveal the answer:</summary>\n",
    "b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Explore the Returned Object: Data, Target Variable, Feature Names**\n",
    "\n",
    "**1. Accessing the Data:**\n",
    "\n",
    "- **Data:** `california_housing.data` holds the features (predictors) as a NumPy array.\n",
    "- **Target Variable:** `california_housing.target` contains the median house values as a NumPy array.\n",
    "- **Feature Names:** `california_housing.feature_names` provides a list of feature names.\n",
    "\n",
    "**2. Understanding the Data:**\n",
    "\n",
    "- **Shape:**\n",
    "   - Print the data shape to see the number of samples and features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20640, 8)\n"
     ]
    }
   ],
   "source": [
    "print(california_housing.data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Data Types:**\n",
    "   - Check data types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float64\n"
     ]
    }
   ],
   "source": [
    "print(california_housing.data.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Previewing Data:**\n",
    "   - View a snippet of data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 8.32520000e+00  4.10000000e+01  6.98412698e+00  1.02380952e+00\n",
      "   3.22000000e+02  2.55555556e+00  3.78800000e+01 -1.22230000e+02]\n",
      " [ 8.30140000e+00  2.10000000e+01  6.23813708e+00  9.71880492e-01\n",
      "   2.40100000e+03  2.10984183e+00  3.78600000e+01 -1.22220000e+02]\n",
      " [ 7.25740000e+00  5.20000000e+01  8.28813559e+00  1.07344633e+00\n",
      "   4.96000000e+02  2.80225989e+00  3.78500000e+01 -1.22240000e+02]\n",
      " [ 5.64310000e+00  5.20000000e+01  5.81735160e+00  1.07305936e+00\n",
      "   5.58000000e+02  2.54794521e+00  3.78500000e+01 -1.22250000e+02]\n",
      " [ 3.84620000e+00  5.20000000e+01  6.28185328e+00  1.08108108e+00\n",
      "   5.65000000e+02  2.18146718e+00  3.78500000e+01 -1.22250000e+02]]\n"
     ]
    }
   ],
   "source": [
    "print(california_housing.data[:5])  # Print first 5 rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Feature Names:**\n",
    "   - Print feature names for clarity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['MedInc', 'HouseAge', 'AveRooms', 'AveBedrms', 'Population', 'AveOccup', 'Latitude', 'Longitude']\n"
     ]
    }
   ],
   "source": [
    "print(california_housing.feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Examining the Target Variable:**\n",
    "\n",
    "- **Shape:**\n",
    "   - Check the target variable's shape:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20640,)\n"
     ]
    }
   ],
   "source": [
    "print(california_housing.target.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Descriptive Statistics:**\n",
    "   - Calculate summary statistics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    20640.000000\n",
      "mean         2.068558\n",
      "std          1.153956\n",
      "min          0.149990\n",
      "25%          1.196000\n",
      "50%          1.797000\n",
      "75%          2.647250\n",
      "max          5.000010\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# DataFrame for 2D, DataSeries for 1D\n",
    "print(pd.Series(california_housing.target).describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. How do you access the target variable values?**\n",
    "   - a) `california_housing.data`\n",
    "   - b) `california_housing.features`\n",
    "   - c) `california_housing.target`\n",
    "\n",
    "<details>\n",
    "<summary>Click to reveal the answer:</summary>\n",
    "    c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. What does the `data` attribute of the returned object contain?**\n",
    "   - a) Feature names\n",
    "   - b) Target variable values\n",
    "   - c) Feature values\n",
    "\n",
    "   <details>\n",
    "<summary>Click to reveal the answer:</summary>\n",
    "    c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. How do you check the data types of the features?**\n",
    "   - a) `california_housing.data.names`\n",
    "   - b) `california_housing.feature_types`\n",
    "   - c) `california_housing.data.dtype`\n",
    " \n",
    "   <details>\n",
    "<summary>Click to reveal the answer:</summary>\n",
    "    c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Print the Data Shape and Basic Information about Features and Target**\n",
    "\n",
    "**1. Accessing Data and Target:**\n",
    "\n",
    "Recall that:\n",
    "\n",
    "* Feature values are stored in `california_housing.data`.\n",
    "* Target variable values (median house prices) are in `california_housing.target`.\n",
    "* Feature names are available in `california_housing.feature_names`.\n",
    "\n",
    "**2. Printing Data Shape:**\n",
    "\n",
    "Use `print()` with the `.shape` attribute to reveal the number of samples (data points) and features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (20640, 8)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Data shape: {california_housing.data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This provides valuable information like:\n",
    "\n",
    "* **Number of data points:** The first element denotes the number of rows (data points) in the dataset.\n",
    "* **Number of features:** The second element represents the number of columns (features) for each data point.\n",
    "\n",
    "**3. Printing Feature Information:**\n",
    "\n",
    "Get some basic statistics about the features:\n",
    "\n",
    "* **Number of features:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 8\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of features: {len(california_housing.feature_names)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Feature names:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature names: MedInc, HouseAge, AveRooms, AveBedrms, Population, AveOccup, Latitude, Longitude\n"
     ]
    }
   ],
   "source": [
    "print(f\"Feature names: {', '.join(california_housing.feature_names)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows all feature names in a readable format.\n",
    "\n",
    "**4. Exploring Target Information:**\n",
    "\n",
    "Print basic information about the target variable:\n",
    "\n",
    "* **Data type:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target data type: float64\n"
     ]
    }
   ],
   "source": [
    "print(f\"Target data type: {california_housing.target.dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Descriptive statistics:**\n",
    "\n",
    "Use pandas to get insights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target variable statistics:\n",
      "count    20640.000000\n",
      "mean         2.068558\n",
      "std          1.153956\n",
      "min          0.149990\n",
      "25%          1.196000\n",
      "50%          1.797000\n",
      "75%          2.647250\n",
      "max          5.000010\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "target_stats = pd.Series(california_housing.target).describe()\n",
    "print(f\"Target variable statistics:\\n{target_stats}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This reveals measures like mean, median, standard deviation, etc., giving you an idea of the target variable's distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. What command shows the number of data points and features in the dataset?**\n",
    "   - a) `print(california_housing.data.size)`\n",
    "   - b) `print(california_housing.data.shape)`\n",
    "   - c) `print(len(california_housing.data))`\n",
    "   \n",
    "<details>\n",
    "<summary>Click to reveal the answer:</summary>\n",
    "    b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. How do you print all feature names?**\n",
    "   - a) `print(california_housing.features)`\n",
    "   - b) `print(california_housing.feature_names)`\n",
    "   - c) `print(list(california_housing.data.columns))`\n",
    "\n",
    "<details>\n",
    "<summary>Click to reveal the answer:</summary>\n",
    "    b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. What information does the `.describe()` method provide about the target variable?**\n",
    "   - a) Data shape\n",
    "   - b) Feature names\n",
    "   - c) Summary statistics like mean, median, standard deviation\n",
    "   \n",
    "<details>\n",
    "<summary>Click to reveal the answer:</summary>\n",
    "    c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Calculate and Interpret Measures of Central Tendeny: Mean, Median, Mode**\n",
    "\n",
    "**1. Defining Central Tendency:**\n",
    "\n",
    "Central tendency measures summarize the center of a data distribution, giving us an idea of where most of the values lie. In the California Housing dataset, these measures tell us where most median house prices fall.\n",
    "\n",
    "**2. Calculating Measures:**\n",
    "\n",
    "- **Mean:** Represents the average value. Use `np.mean(california_housing.target)` (NumPy) or `pd.Series(california_housing.target).mean()` (pandas).\n",
    "- **Median:** Indicates the middle value after sorting the data. Use `np.median(california_housing.target)` or `pd.Series(california_housing.target).median()`.\n",
    "- **Mode:** Represents the most frequent value. Use `np.mode(california_housing.target)` or `pd.Series(california_housing.target).mode()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:\n",
      "2.068558169089147\n",
      "2.0685581690891843\n",
      "Median:\n",
      "1.797\n",
      "1.797\n",
      "Mode: only using pandas. Numpy does not have mode()\n",
      "0    5.00001\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "print('Mean:')\n",
    "print(np.mean(california_housing.target))\n",
    "print(pd.Series(california_housing.target).mean())\n",
    "\n",
    "print('Median:')\n",
    "print(np.median(california_housing.target))\n",
    "print(pd.Series(california_housing.target).median())\n",
    "\n",
    "print('Mode: only using pandas. Numpy does not have mode()')\n",
    "print(pd.Series(california_housing.target).mode())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Interpreting Results:**\n",
    "\n",
    "- **Mean:** Sensitive to outliers. A single highly deviating value can significantly affect the mean.\n",
    "- **Median:** Less influenced by outliers, making it a robust measure for skewed distributions.\n",
    "- **Mode:** Useful for categorical data or data with multiple peaks.\n",
    "\n",
    "**4. Example in Context:**\n",
    "\n",
    "- Imagine the mean house price is significantly higher than the median. This might indicate a few extremely expensive houses skewing the average.\n",
    "- If the mode differs from both the mean and median, it suggests the data has multiple clusters or peaks in its distribution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Which measure is least affected by outliers?**\n",
    "   - a) Mean\n",
    "   - b) Median\n",
    "   - c) Mode\n",
    "\n",
    "<details>\n",
    "<summary>Click to reveal the answer:</summary>\n",
    "    b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. What does the mode tell us about the data?**\n",
    "   - a) Total sum of all values\n",
    "   - b) Most frequent value\n",
    "   - c) Range of values\n",
    "   \n",
    "<details>\n",
    "<summary>Click to reveal the answer:</summary>\n",
    "    b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. How can you calculate the mean of the target variable using pandas?**\n",
    "   - a) `california_housing.target.mean()`\n",
    "   - b) `pd.Series(california_housing.target).mean()`\n",
    "   - c) `np.average(california_housing.target)`\n",
    "\n",
    "<details>\n",
    "<summary>Click to reveal the answer:</summary>\n",
    "    b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Analyze Measures of Dispersion: Range, Variance, Standard Deviation**\n",
    "\n",
    "While central tendency measures tell us **where** most data points lie, measures of dispersion inform us **how spread out** the data is. In the California Housing data, they reveal how much median house prices vary.\n",
    "\n",
    "**1. Defining Dispersion:**\n",
    "\n",
    "Dispersion measures quantify the variability or spread of data points around the central tendency. Higher values indicate greater spread, and lower values suggest data concentrated around the center.\n",
    "\n",
    "**2. Calculating Measures:**\n",
    "\n",
    "- **Range:** Simplest measure, calculated as the difference between the largest and smallest values. Use `np.max(california_housing.target) - np.min(california_housing.target)`.\n",
    "- **Variance:** Average squared deviation of each data point from the mean. Use `np.var(california_housing.target)` or `pd.Series(california_housing.target).var()`.\n",
    "- **Standard Deviation (SD):** Square root of the variance, representing the typical distance of data points from the mean. Use `np.std(california_housing.target)` or `pd.Series(california_housing.target).std()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Range:\n",
      "4.85002\n",
      "Variance\n",
      "1.3315503000818076\n",
      "Standard Deviation\n",
      "1.1539282040412253\n"
     ]
    }
   ],
   "source": [
    "print('Range:')\n",
    "print(np.max(california_housing.target) - np.min(california_housing.target))\n",
    "print('Variance')\n",
    "print(np.var(california_housing.target))\n",
    "print('Standard Deviation')\n",
    "print(np.std(california_housing.target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Interpreting Results:**\n",
    "\n",
    "- **Range:** Easy to understand but sensitive to outliers and doesn't consider all data points.\n",
    "- **Variance:** Highly influenced by outliers due to squaring. Difficult to interpret directly.\n",
    "- **SD:** Preferred measure of dispersion, captures typical deviation from the mean, easier to interpret than variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Example in Context:**\n",
    "\n",
    "- A high range could indicate a mix of very affordable and extremely expensive houses in the dataset.\n",
    "- A large variance compared to the mean suggests significant spread in house prices, with both low and high values.\n",
    "- A low SD implies most house prices are close to the average, with less variation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Which measure is least affected by outliers?**\n",
    "   - a) Range\n",
    "   - b) Variance\n",
    "   - c) Standard Deviation\n",
    "\n",
    "<details>\n",
    "<summary>Click to reveal the answer:</summary>\n",
    "    c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. What does the standard deviation tell us?**\n",
    "   - a) Difference between largest and smallest values\n",
    "   - b) Total sum of squared deviations from the mean\n",
    "   - c) Typical distance of data points from the mean\n",
    "\n",
    "<details>\n",
    "<summary>Click to reveal the answer:</summary>\n",
    "    c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. How can you calculate the standard deviation of the features using NumPy?**\n",
    "   - a) `np.var(california_housing.data)`\n",
    "   - b) `california_housing.features.std()`\n",
    "   - c) `np.std(california_housing.data, axis=0)`\n",
    "\n",
    "<details>\n",
    "<summary>Click to reveal the answer:</summary>\n",
    "    c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Investigate Correlations between Features and Target Variable using Pearson's Correlation Coefficient**\n",
    "\n",
    "**1. Understanding Correlation:**\n",
    "\n",
    "Correlation measures the **linear relationship** between two variables. In the California Housing data, it tells us how much the target variable (median house price) changes relative to changes in each feature.\n",
    "\n",
    "**2. Pearson's Correlation Coefficient:**\n",
    "\n",
    "This common metric quantifies the strength and direction of the linear relationship:\n",
    "\n",
    "* **Values range from -1 to 1:**\n",
    "    * Positive values indicate a positive correlation (features increase together with the target).\n",
    "    * Negative values indicate a negative correlation (features decrease as the target increases).\n",
    "    * Closer to 0 implies weaker or no linear relationship.\n",
    "* **Interpretation:**\n",
    "    * Strong correlations (>0.7) suggest potential influence of the feature on the target variable.\n",
    "    * Weak correlations (<0.3) might offer less predictive power.\n",
    "\n",
    "**3. Calculating Correlations in Python:**\n",
    "\n",
    "Use `pd.DataFrame.corr()` with the California Housing data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          0         1         2         3         4         5         6  \\\n",
      "0  1.000000 -0.119034  0.326895 -0.062040  0.004834  0.018766 -0.079809   \n",
      "1 -0.119034  1.000000 -0.153277 -0.077747 -0.296244  0.013191  0.011173   \n",
      "2  0.326895 -0.153277  1.000000  0.847621 -0.072213 -0.004852  0.106389   \n",
      "3 -0.062040 -0.077747  0.847621  1.000000 -0.066197 -0.006181  0.069721   \n",
      "4  0.004834 -0.296244 -0.072213 -0.066197  1.000000  0.069863 -0.108785   \n",
      "5  0.018766  0.013191 -0.004852 -0.006181  0.069863  1.000000  0.002366   \n",
      "6 -0.079809  0.011173  0.106389  0.069721 -0.108785  0.002366  1.000000   \n",
      "7 -0.015176 -0.108197 -0.027540  0.013344  0.099773  0.002476 -0.924664   \n",
      "\n",
      "          7  \n",
      "0 -0.015176  \n",
      "1 -0.108197  \n",
      "2 -0.027540  \n",
      "3  0.013344  \n",
      "4  0.099773  \n",
      "5  0.002476  \n",
      "6 -0.924664  \n",
      "7  1.000000  \n"
     ]
    }
   ],
   "source": [
    "correlations = pd.DataFrame(california_housing.data).corr()\n",
    "print(correlations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MedInc                0.688075\n",
      "HouseAge              0.105623\n",
      "AveRooms              0.151948\n",
      "AveBedrms            -0.046701\n",
      "Population           -0.024650\n",
      "AveOccup             -0.023737\n",
      "Latitude             -0.144160\n",
      "Longitude            -0.045967\n",
      "median_house_value    1.000000\n",
      "Name: median_house_value, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Extract features and target variable\n",
    "features = pd.DataFrame(california_housing.data, columns=california_housing.feature_names)\n",
    "target = pd.Series(california_housing.target, name=\"median_house_value\")\n",
    "\n",
    "# Concatenate features and target\n",
    "data_with_target = pd.concat([features, target], axis=1)\n",
    "\n",
    "# Calculate correlations with actual names\n",
    "correlations = data_with_target.corr()\n",
    "\n",
    "print(correlations.iloc[:, -1]) #.iloc[:, -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "This will create a table showing Pearson's correlation coefficient between each feature and the target variable.\n",
    "\n",
    "**4. Analyzing Results:**\n",
    "\n",
    "- Focus on features with high absolute values (closer to 1 or -1).\n",
    "- Identify features with positive correlations if you wish to predict **higher** house prices.\n",
    "- Identify features with negative correlations if you wish to predict **lower** house prices.\n",
    "- Remember, correlation doesn't imply causation. Investigate further before drawing conclusions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. What is the range of values for Pearson's correlation coefficient?**\n",
    "   - a) 0 to 1\n",
    "   - b) -1 to 1\n",
    "   - c) -0.5 to 0.5\n",
    "   \n",
    "<details>\n",
    "<summary>Click to reveal the answer:</summary>\n",
    "    b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. A coefficient of -0.8 between a feature and the target indicates:**\n",
    "   - a) Strong positive correlation\n",
    "   - b) Strong negative correlation\n",
    "   - c) Weak correlation\n",
    "\n",
    "<details>\n",
    "<summary>Click to reveal the answer:</summary>\n",
    "    b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. How can you calculate the correlation among all features?**\n",
    "   - a) `np.corrcoef(california_housing.data, california_housing.target)`\n",
    "   - b) `pd.DataFrame(california_housing.data).corr()`\n",
    "   - c) `california_housing.data.corrwith(california_housing.target)`\n",
    "\n",
    "<details>\n",
    "<summary>Click to reveal the answer:</summary>\n",
    "    b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:3px;border-width:0;color:blue;background-color:blue\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Preprocessing Data**\n",
    "\n",
    "### 1. Handling Missing Values:\n",
    "\n",
    "Handling missing values is a crucial step in the data preprocessing phase. In the California housing dataset, we'll identify and address any missing values to ensure the integrity and reliability of the data.\n",
    "\n",
    "**Identifying Missing Values:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MedInc        0\n",
      "HouseAge      0\n",
      "AveRooms      0\n",
      "AveBedrms     0\n",
      "Population    0\n",
      "AveOccup      0\n",
      "Latitude      0\n",
      "Longitude     0\n",
      "dtype: int64\n",
      "Series([], dtype: int64)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "# Load the dataset\n",
    "housing = fetch_california_housing()\n",
    "df = pd.DataFrame(housing.data, columns=housing.feature_names)\n",
    "\n",
    "# Check for missing values in the dataset\n",
    "missing_values = df.isnull().sum()\n",
    "\n",
    "# Display features with missing values\n",
    "features_with_missing_values = missing_values[missing_values > 0]\n",
    "# Check for missing values\n",
    "print(missing_values)  # Count missing values in each feature\n",
    "print(features_with_missing_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `missing_values` variable contains the count of missing values for each feature in the dataset. By examining `features_with_missing_values`, we can identify which features have missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Series:\n",
      "0    1.0\n",
      "1    2.0\n",
      "2    3.0\n",
      "3    NaN\n",
      "4    5.0\n",
      "5    6.0\n",
      "6    NaN\n",
      "7    8.0\n",
      "8    9.0\n",
      "9    NaN\n",
      "dtype: float64\n",
      "\n",
      "Series after mean imputation:\n",
      "0    1.000000\n",
      "1    2.000000\n",
      "2    3.000000\n",
      "3    4.857143\n",
      "4    5.000000\n",
      "5    6.000000\n",
      "6    4.857143\n",
      "7    8.000000\n",
      "8    9.000000\n",
      "9    4.857143\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Impute missing values with mean of each feature\n",
    "\n",
    "# Create a series with 10 values and 3 missing values\n",
    "dummy_data = pd.Series([1, 2, 3, None, 5, 6, None, 8, 9, None])\n",
    "\n",
    "# Print the series before imputation\n",
    "print(\"Original Series:\")\n",
    "print(dummy_data)\n",
    "\n",
    "# Impute missing values with the mean of the series\n",
    "data_mean_imputed = dummy_data.fillna(dummy_data.mean())\n",
    "\n",
    "# Print the imputed series\n",
    "print(\"\\nSeries after mean imputation:\")\n",
    "print(data_mean_imputed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Strategies for Handling Missing Values:**\n",
    "\n",
    "1. **Imputation:**\n",
    "   - For numerical features with missing values, imputation can be performed by replacing missing values with the mean, median, or mode of the feature.\n",
    "\n",
    "   ```python\n",
    "   # Impute missing values with the mean\n",
    "   df['numerical_feature'].fillna(df['numerical_feature'].mean(), inplace=True)\n",
    "   ```\n",
    "\n",
    "2. **Removal:**\n",
    "   - If the missing values are deemed significant and imputation is not appropriate, removal of rows or columns with missing values may be considered.\n",
    "\n",
    "   ```python\n",
    "   # Remove rows with missing values\n",
    "   df.dropna(subset=['feature_with_missing_values'], inplace=True)\n",
    "   ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Categorical Feature Imputation:**\n",
    "   - For categorical features, missing values can be replaced with the most frequent category.\n",
    "\n",
    "   ```python\n",
    "   # Impute missing categorical values with the most frequent category\n",
    "   df['categorical_feature'].fillna(df['categorical_feature'].mode()[0], inplace=True)\n",
    "   ```\n",
    "\n",
    "**Assessing the Impact:**\n",
    "\n",
    "After handling missing values, it's essential to assess the impact on the dataset's overall structure and statistics.\n",
    "\n",
    "```python\n",
    "# Check for missing values again after handling\n",
    "remaining_missing_values = df.isnull().sum().sum()\n",
    "```\n",
    "\n",
    "The `remaining_missing_values` variable gives the total count of remaining missing values after applying the chosen strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summary:**\n",
    "\n",
    "Handling missing values is a crucial step to ensure the dataset's reliability and suitability for analysis. The chosen strategy depends on the nature of the missing data and the specific requirements of the analysis or modeling task.\n",
    "\n",
    "This process ensures that the dataset is more robust and ready for subsequent phases of the data-driven cycle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. What is the main disadvantage of simply deleting rows with missing values?**\n",
    "    - a) Reduces training data size\n",
    "    - b) Introduces bias in the analysis\n",
    "    - c) Increases model complexity\n",
    "    \n",
    "<details>\n",
    "<summary>Click to reveal the answer:</summary>\n",
    "    a) and b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Which imputation technique replaces missing values with the average value of the feature?**\n",
    "    - a) KNN imputation\n",
    "    - b) Mean imputation\n",
    "    - c) Model-based imputation\n",
    "    \n",
    "<details>\n",
    "<summary>Click to reveal the answer:</summary>\n",
    "    b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. When choosing an imputation method, it's important to consider:**\n",
    "    - a) The color of the missing values\n",
    "    - b) The type of missing data\n",
    "    - c) The model's favorite food\n",
    "    \n",
    "<details>\n",
    "<summary>Click to reveal the answer:</summary>\n",
    "    b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2. Outliers: Identifying and Taming Extremes in California Housing Data**\n",
    "\n",
    "Outliers, data points significantly deviating from the typical pattern, can pose challenges for analyzing the California Housing Price dataset. Fortunately, we have tools to detect and handle these unusual values, ensuring our models learn from accurate representations of the data.\n",
    "\n",
    "**2.1 Understanding Outliers:**\n",
    "\n",
    "- **Definition:** Data points that fall far outside the expected range of a feature's distribution.\n",
    "- **Causes:** Measurement errors, extreme values within the population, or unexpected events.\n",
    "- **Impact:** Can distort statistical measures, mislead models, and negatively affect analysis results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.2 Detection Methods:**\n",
    "\n",
    "- **Statistical approaches:** \n",
    "    - **Z-scores:** Identify data points beyond specified standard deviation thresholds.\n",
    "    - **Boxplots:** Outliers fall outside the whiskers indicating the interquartile range.\n",
    "- **Visualization:** Histograms and scatter plots can reveal unusual clusters or extreme points deviating from the main distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        MedInc\n",
      "131    11.6017\n",
      "409    10.0825\n",
      "510    11.8603\n",
      "511    13.4990\n",
      "512    12.2138\n",
      "514    12.3804\n",
      "923     9.7194\n",
      "977    10.9506\n",
      "986    10.3203\n",
      "1541    9.5862\n",
      "1561    9.7037\n",
      "1563   10.3345\n",
      "1564   12.5915\n",
      "1566   15.0001\n",
      "1574    9.8708\n",
      "1582   10.7372\n",
      "1583   13.4883\n",
      "1586   12.2478\n",
      "1591   10.4549\n",
      "1593   10.3224\n",
      "1602   10.3713\n",
      "1617   11.7064\n",
      "1621   11.3421\n",
      "1636   10.9405\n",
      "1637   10.3416\n",
      "1644   10.5815\n",
      "1645   13.2949\n",
      "1646   13.1499\n",
      "2213   10.5144\n",
      "2826   10.0263\n",
      "...        ...\n",
      "18283  10.3591\n",
      "18287  10.3329\n",
      "18298  10.3894\n",
      "18325  10.3942\n",
      "18327  10.9508\n",
      "18341  12.5902\n",
      "18347  11.0492\n",
      "18348  10.4760\n",
      "18351  10.7237\n",
      "18352  12.0372\n",
      "18353  10.7355\n",
      "18354  13.1867\n",
      "18355  11.8060\n",
      "18356  10.4277\n",
      "18360  10.0259\n",
      "18361  12.1387\n",
      "18362  12.9792\n",
      "18363  13.8093\n",
      "18366  11.3283\n",
      "18501  15.0001\n",
      "18504  15.0001\n",
      "18505   9.5908\n",
      "18619  10.0968\n",
      "19006  10.2264\n",
      "20163  12.6320\n",
      "20376  10.2614\n",
      "20380  10.1597\n",
      "20389  10.0595\n",
      "20426  10.0472\n",
      "20436  12.5420\n",
      "\n",
      "[345 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Calculate Z-scores for MedInc and HouseAge\n",
    "z_scores = np.abs((df[['MedInc']] - df[['MedInc']].mean()) / df[['MedInc']].std())\n",
    "\n",
    "# Identify potential outliers (e.g., Z-scores > 3)\n",
    "outliers = df[np.any(z_scores > 3, axis=1)]\n",
    "print(outliers[['MedInc']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Create boxplots for MedInc and HouseAge\n",
    "df[['MedInc']].boxplot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.3 Handling Outliers:**\n",
    "\n",
    "- **Removal:** Simplest option but can potentially discard valuable information and introduce bias.\n",
    "- **Capping:** Limit outlier values to a chosen threshold, preserving data points while reducing their influence.\n",
    "- **Winsorization:** Replace outliers with values at the tails of the distribution (e.g., quartiles).\n",
    "- **Transformation:** Apply techniques like log scaling or power transformations to reduce skewness and outlier impact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#_no_outliers\n",
    "df = df[~np.any(z_scores > 3, axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.4 Choosing the Right Approach:**\n",
    "\n",
    "- Consider the **context and nature of the outliers:** Are they errors, valid extremes, or anomalies?\n",
    "- Evaluate the **impact on model performance:** Compare results with and without outlier handling.\n",
    "- **Document and justify** your chosen approach for transparency and reproducibility."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. What can be a disadvantage of removing outliers?**\n",
    "    - a) Reduced model accuracy\n",
    "    - b) Decreased training data size\n",
    "    - c) Improved data visualization\n",
    "    \n",
    "<details>\n",
    "<summary>Click to reveal the answer:</summary>\n",
    "    a) and b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Z-scores identify outliers that are:**\n",
    "    - a) Within the interquartile range\n",
    "    - b) Further than a certain number of standard deviations from the mean\n",
    "    - c) Colored differently on a boxplot\n",
    "\n",
    "<details>\n",
    "<summary>Click to reveal the answer:</summary>\n",
    "    b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Capping is a good technique for handling:**\n",
    "    - a) Measurement errors\n",
    "    - b) Valid extreme values within the population\n",
    "    - c) Missing data points\n",
    "\n",
    "<details>\n",
    "<summary>Click to reveal the answer:</summary>\n",
    "    b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Encoding Categorical Variables: Making Features Models Ready\n",
    "\n",
    "In the California housing dataset, there are apparently no categorical variables. To utilize these categorical features in machine learning models though, encoding categorical variables into numerical format is necessary. We'll explore common encoding techniques for handling categorical variables.\n",
    "\n",
    "**3.1 Identifying Categorical Variables:**\n",
    "The `categorical_features` variable contains the names of columns that have categorical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index([], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "categorical_features = df.columns[df.nunique() < 5]\n",
    "print(categorical_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.2 One-Hot Encoding:**\n",
    "\n",
    "One-hot encoding is a widely used technique to convert categorical variables into a binary matrix, where each category becomes a binary column.\n",
    "\n",
    "```python\n",
    "# Perform one-hot encoding\n",
    "df_encoded = pd.get_dummies(df, columns=categorical_features, drop_first=True)\n",
    "```\n",
    "\n",
    "The `df_encoded` DataFrame now includes binary columns for each category in the original categorical features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.3 Label Encoding (Alternative):**\n",
    "\n",
    "Another encoding technique is label encoding, where each category is assigned a unique integer label. This is suitable for ordinal categorical variables.\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Initialize LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Apply label encoding to a specific categorical feature\n",
    "df['ordinal_categorical_feature_encoded'] = label_encoder.fit_transform(df['ordinal_categorical_feature'])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Handling Ordinal Categorical Variables:**\n",
    "\n",
    "For ordinal categorical variables (those with a clear order), preserving the order during encoding is essential. This can be achieved through custom mapping.\n",
    "\n",
    "```python\n",
    "# Define mapping for an ordinal categorical feature\n",
    "ordinal_mapping = {'low': 0, 'medium': 1, 'high': 2}\n",
    "\n",
    "# Apply mapping to the ordinal categorical feature\n",
    "df['ordinal_categorical_feature_encoded'] = df['ordinal_categorical_feature'].map(ordinal_mapping)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.5 Assessing the Impact:**\n",
    "\n",
    "It's crucial to assess the impact of encoding on the dataset's structure and dimensionality.\n",
    "\n",
    "```python\n",
    "# Compare the dimensions before and after encoding\n",
    "original_dimensions = df.shape\n",
    "encoded_dimensions = df_encoded.shape\n",
    "```\n",
    "\n",
    "The variables `original_dimensions` and `encoded_dimensions` provide the number of rows and columns before and after encoding, respectively.\n",
    "\n",
    "**3.6 Summary:**\n",
    "\n",
    "Encoding categorical variables is a critical step in preparing the dataset for machine learning models. The choice of encoding technique depends on the nature of the categorical data and the requirements of the modeling task. Whether using one-hot encoding, label encoding, or custom mapping, the goal is to represent categorical information in a format suitable for numerical analysis.\n",
    "\n",
    "This ensures that the dataset is appropriately structured for subsequent stages of the data-driven cycle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4. Feature Scaling: Balancing the Playing Field in California Housing Data**\n",
    "\n",
    "The California Housing Price dataset features diverse measurements in different units, such as income, distance, and area. Before training machine learning models, these features need to be on a similar scale to avoid biases and improve learning efficiency. Here's where feature scaling comes to the rescue!\n",
    "\n",
    "**4.1 Why Scale Features?**\n",
    "\n",
    "- **Uneven scales:** Features with larger values can dominate models, masking the influence of smaller features.\n",
    "- **Gradient descent optimization:** Models using gradient descent struggle with features on vastly different scales, slowing down training.\n",
    "- **Standardized coefficients:** Scaling allows interpreting model coefficients directly as the impact of a feature on the target variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.2 Common Scaling Techniques:**\n",
    "\n",
    "- **Standardization:** Transforms features to have zero mean and unit variance (e.g., using `sklearn.preprocessing.StandardScaler`).\n",
    "- **Normalization:** Rescales features to a specific range, often 0-1 (e.g., using `sklearn.preprocessing.MinMaxScaler`).\n",
    "- **Robust Scalers:** Handle outliers more effectively than StandardScaler (e.g., `sklearn.preprocessing.RobustScaler`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MedInc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2.029500e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-6.485683e-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.000025e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-2.018121e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-7.402370e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-1.440240e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>5.784291e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>3.624795e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             MedInc\n",
       "count  2.029500e+04\n",
       "mean  -6.485683e-16\n",
       "std    1.000025e+00\n",
       "min   -2.018121e+00\n",
       "25%   -7.402370e-01\n",
       "50%   -1.440240e-01\n",
       "75%    5.784291e-01\n",
       "max    3.624795e+00"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Select features to scale\n",
    "features_to_scale = ['MedInc']\n",
    "\n",
    "# Create a scaler object\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler to the training data (assuming you've split your data)\n",
    "scaler.fit(df[features_to_scale])\n",
    "\n",
    "# Transform the selected features (both training and testing sets)\n",
    "df[features_to_scale] = scaler.transform(df[features_to_scale])\n",
    "df[features_to_scale].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MedInc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>20295.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.357638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.177218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.226458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.332115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.460143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             MedInc\n",
       "count  20295.000000\n",
       "mean       0.357638\n",
       "std        0.177218\n",
       "min        0.000000\n",
       "25%        0.226458\n",
       "50%        0.332115\n",
       "75%        0.460143\n",
       "max        1.000000"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Create a MinMaxScaler object\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Fit and transform the selected features, as done for standardization\n",
    "df[features_to_scale] = scaler.fit_transform(df[features_to_scale])\n",
    "df[features_to_scale].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MedInc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>20295.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.109219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.758361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-1.421207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.452133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.547867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.858054</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             MedInc\n",
       "count  20295.000000\n",
       "mean       0.109219\n",
       "std        0.758361\n",
       "min       -1.421207\n",
       "25%       -0.452133\n",
       "50%        0.000000\n",
       "75%        0.547867\n",
       "max        2.858054"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "# Create a RobustScaler object\n",
    "scaler = RobustScaler()\n",
    "\n",
    "# Fit and transform the selected features, as done for standardization\n",
    "df[features_to_scale] = scaler.fit_transform(df[features_to_scale])\n",
    "df[features_to_scale].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.3 Choosing the Right Technique:**\n",
    "\n",
    "- **Feature distribution:** Gaussian distributions benefit from standardization, while others might prefer Normalization.\n",
    "- **Outliers:** Robust Scalers are better suited for data with outliers that might distort StandardScaler.\n",
    "- **Model type:** Some models like KNN are sensitive to feature scaling, requiring careful selection.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.4 Impact on Machine Learning:**\n",
    "\n",
    "- **Improved convergence:** Models learn faster and reach better optima with scaled features.\n",
    "- **Reduced bias:** Smaller features have a fairer influence, leading to more robust models.\n",
    "- **Simplified interpretation:** Standardized coefficients offer an intuitive understanding of feature importance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. What does standardizing a feature achieve?**\n",
    "    - a) Rescaling values to a specific range\n",
    "    - b) Setting the mean and variance to zero\n",
    "    - c) Increasing the range of values\n",
    "    \n",
    "<details>\n",
    "<summary>Click to reveal the answer:</summary>\n",
    "    b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Which feature scaling technique is robust to outliers?**\n",
    "    - a) MinMaxScaler\n",
    "    - b) StandardScaler\n",
    "    - c) RobustScaler\n",
    "    \n",
    "<details>\n",
    "<summary>Click to reveal the answer:</summary>\n",
    "    c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. How does feature scaling improve model performance?**\n",
    "    - a) By increasing the number of features\n",
    "    - b) By reducing bias and improving convergence\n",
    "    - c) By making the model more complex\n",
    "    \n",
    "<details>\n",
    "<summary>Click to reveal the answer:</summary>\n",
    "    b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **5. Feature Engineering: Enriching the Data Further**\n",
    "\n",
    "Feature engineering involves creating new features or transforming existing ones to enhance the performance and interpretability of machine learning models. In the California housing dataset, we'll explore opportunities for feature engineering.\n",
    "\n",
    "**5.1 Combining Features:**\n",
    "Create a new feature by combining existing ones. For example, a 'Total Rooms' feature can be created by summing the 'Total Bedrooms' and 'Total Bathrooms' features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    20295.000000\n",
       "mean         6.480666\n",
       "std          2.881082\n",
       "min          1.388889\n",
       "25%          5.478925\n",
       "50%          6.257212\n",
       "75%          7.070160\n",
       "max        167.545455\n",
       "Name: TotalRooms, dtype: float64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a new feature 'Total Rooms'\n",
    "df['TotalRooms'] = df['AveRooms'] + df['AveBedrms']\n",
    "df['TotalRooms'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5.2 Feature Interaction:**\n",
    "   Create new features by combining existing ones. For example, you can create a feature representing the average rooms per bedroom:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    20295.000000\n",
       "mean         4.937102\n",
       "std          1.118227\n",
       "min          1.000000\n",
       "25%          4.156819\n",
       "50%          4.898851\n",
       "75%          5.657454\n",
       "max         10.000000\n",
       "Name: AveRoomsPerBedroom, dtype: float64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Feature interaction: Average rooms per bedroom\n",
    "df['AveRoomsPerBedroom'] = df['AveRooms'] / df['AveBedrms'].replace(0, 1)  # Avoid division by zero\n",
    "df['AveRoomsPerBedroom'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5.3 Geospatial Features:**\n",
    "   Extract additional information from latitude and longitude. You can create a feature indicating the proximity to the coast:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    20295.000000\n",
       "mean         0.588963\n",
       "std          0.492034\n",
       "min          0.000000\n",
       "25%          0.000000\n",
       "50%          1.000000\n",
       "75%          1.000000\n",
       "max          1.000000\n",
       "Name: NearCoast, dtype: float64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Proximity to coast based on latitude\n",
    "df['NearCoast'] = np.where(df['Latitude'] < 36.5, 1, 0)\n",
    "df['NearCoast'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5.4 Population Density:**\n",
    "   Calculate a new feature representing population density:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    20295.000000\n",
       "mean       290.400431\n",
       "std        247.628239\n",
       "min          0.211403\n",
       "25%        146.908329\n",
       "50%        227.319520\n",
       "75%        354.798474\n",
       "max       6770.139566\n",
       "Name: PopulationDensity, dtype: float64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Population density\n",
    "df['PopulationDensity'] = df['Population'] / df['AveRooms']\n",
    "df['PopulationDensity'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are just a few examples of feature engineering techniques. The choice of which techniques to apply depends on the characteristics of your data and the specific requirements of your machine learning task. Always monitor the impact of feature engineering on model performance and adjust accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:6px;border-width:0;color:red;background-color:red\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Practice Exercises: California Housing Price Dataset**\n",
    "\n",
    "1. **Accessing Data:** Download the California Housing Price dataset and import it into Python using `scikit-learn`. Print the shape of the data and the names of the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Exploring Features:** Print the first few rows of the data to get a glimpse of the feature values. Calculate and interpret the minimum, maximum, and mean values for each feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Target Variable:** Analyze the target variable (median house price) by calculating and interpreting measures like mean, median, mode, and range. Visualize the distribution of target values using a histogram or boxplot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. **Central Tendency:** Calculate and interpret the mean, median, and mode for all features. Compare and contrast these measures for different features to understand their central tendency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. **Dispersion:** Calculate and interpret the range, variance, and standard deviation for all features. Identify features with high variability and explore potential reasons for their spread."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. **Combining Measures:** Analyze the relationship between central tendency and dispersion measures for each feature. Discuss how they provide complementary insights into the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. **Feature Correlations:** Calculate and interpret Pearson's correlation coefficient between each feature and the target variable. Discuss how these correlations might impact model building and feature selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. **Visualizing Correlations:** Create scatter plots between features and the target variable to visualize their relationships. Identify potential non-linear relationships or outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. **Handling Missing Values:** Load the California Housing dataset and print the number of missing values in each feature.\n",
    "   - Impute missing values using mean imputation for all features.\n",
    "   - Impute missing values using KNN imputation with `n_neighbors=5` for the 'MedInc' feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. **Outlier Detection:** Calculate Z-scores for all features and print data points with Z-scores greater than 3 in absolute value.\n",
    "   - Create boxplots for 'HouseAge' to visualize potential outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. **Feature Scaling:**\n",
    "\n",
    "**Standardization:**\n",
    "   - Standardize the features 'AveRooms', and 'Population' using StandardScaler.\n",
    "   - Print the mean and standard deviation of these features before and after standardization.\n",
    "\n",
    "**Normalization:**\n",
    "   - Normalize the features 'MedInc', and 'HouseAge' using MinMaxScaler.\n",
    "   - Print the minimum and maximum values of these features before and after normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:6px;border-width:0;color:blue;background-color:blue\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **A Reference to Library Functions Used**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1. NumPy Functions Used in Example Code**\n",
    "\n",
    "**Here's a table summarizing the NumPy functions used in the code examples:**\n",
    "\n",
    "| Function            | Purpose                                                 | Example Usage                                               |\n",
    "|---------------------|----------------------------------------------------------|-----------------------------------------------------------|\n",
    "| `abs`               | Calculates the absolute value of elements                | `z_scores = np.abs((data - data.mean()) / data.std())`    |\n",
    "| `any`               | Checks if any elements in an array satisfy a condition   | `outliers = data[np.any(z_scores > 3, axis=1)]`            |\n",
    "| `mean`              | Calculates the mean of elements along a specified axis   | `data = data.clip(lower=data.mean() - 3*data.std(), ...)`  |\n",
    "| `std`               | Calculates the standard deviation of elements            | (Same as above)                                            |\n",
    "| `clip`              | Caps values in an array within a specified range         | (Same as above)                                            |\n",
    "| `quantile`          | Calculates the specified quantiles of elements           | `q1 = data[col].quantile(0.025)`                           |\n",
    "| `iqr`               | Calculates the interquartile range (IQR) of elements    | `iqr = q3 - q1`                                             |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2. Pandas Functions Used in Example Code**\n",
    "\n",
    " **Here's a table summarizing the pandas functions used in the code examples:**\n",
    "\n",
    "| Function          | Purpose                                                   | Example Usage                                                  |\n",
    "|--------------------|------------------------------------------------------------|----------------------------------------------------------------|\n",
    "| `isnull`          | Detects missing values (NaNs)                               | `data.isnull().sum()`                                           |\n",
    "| `sum`              | Calculates the sum of values along a specified axis          | (Same as above)                                                 |\n",
    "| `fillna`           | Fills missing values with specified values                  | `data_mean_imputed = data.fillna(data.mean())`                 |\n",
    "| `boxplot`          | Creates boxplots for visualization of distributions          | `data.boxplot()`                                                |\n",
    "| `clip`             | Caps values within a specified range                         | `data = data.clip(lower=data.mean() - 3*data.std(), ...)`      |\n",
    "| `quantile`         | Calculates specified quantiles of values                     | `q1 = data[col].quantile(0.025)`                                |\n",
    "| `DataFrame`        | Creates a DataFrame from array-like data                     | `data = pd.DataFrame(housing.data, columns=housing.feature_names)` |\n",
    "| `matrix`           | Visualizes missingness patterns (using missingno library)    | `msno.matrix(data)`                                             |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3. Matplotlib and Seaborn Functions Used in Example Code**\n",
    "\n",
    "| Library | Function | Purpose | Variation (if any) | Example Usage |\n",
    "|---|---|---|---|---|\n",
    "| **Matplotlib** | `plt.scatter()` | Creates a scatter plot | x and y data, marker, color | `plt.scatter(boston[\"TAX\"], boston[\"PRICE\"], c=\"blue\")` |\n",
    "| | `plt.bar()` | Creates a bar chart | x and y data, color, label | `plt.bar(chas_counts.index, chas_counts.values, color=[\"red\", \"green\"])` |\n",
    "| | `plt.xlabel()` | Sets label for x-axis | Text | `plt.xlabel(\"Property Tax\")` |\n",
    "| | `plt.ylabel()` | Sets label for y-axis | Text | `plt.ylabel(\"Price\")` |\n",
    "| | `plt.title()` | Sets title for plot | Text | `plt.title(\"Distribution of CHAS categories\")` |\n",
    "| | `plt.show()` | Displays the plot | | `plt.show()` |\n",
    "| **Seaborn** | `sns.relplot()` | Creates a relational plot (scatter with lines) | x and y data, hue (categorical variable), linestyle | `sns.relplot(x=\"RM\", y=\"MEDV\", hue=\"CHAS\", linestyle=\"--\", data=boston)` |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4. Scikit-learn Functions Used in Example Code**\n",
    "\n",
    " **Here's a table summarizing the scikit-learn functions used in the code examples:**\n",
    "\n",
    "| Function           | Purpose                                       | Example Usage                                                       |\n",
    "|---------------------|------------------------------------------------|-------------------------------------------------------------------|\n",
    "| `fetch_california_housing` | Loads the California Housing Price dataset  | `housing = fetch_california_housing()`                            |\n",
    "| `SimpleImputer`    | Imputes missing values                       | `imputer = SimpleImputer(strategy='mean')`                        |\n",
    "| `KNNImputer`       | Imputes missing values using KNN              | `imputer = KNNImputer(n_neighbors=5)`                              |\n",
    "| `StandardScaler`    | Standardizes features to have zero mean and unit variance | `scaler = StandardScaler()`                                       |\n",
    "| `MinMaxScaler`      | Normalizes features to a specific range       | `scaler = MinMaxScaler()`                                         |\n",
    "| `RobustScaler`      | Scales features robustly to outliers          | `scaler = RobustScaler()`                                           |\n",
    "| `cross_val_score`  | Evaluates model performance using cross-validation | `scores = cross_val_score(model, data, target, cv=5)`              |\n",
    "| `LinearRegression`  | Trains a linear regression model               | `model = LinearRegression()`                                      |\n",
    "| `clip`             | Caps values within a certain range            | `data = data.clip(lower=-3, upper=3)`                              |\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
